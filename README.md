# AC-ETL-Project
Algonquin College Foundation of Transforming and Storing Data Project

# Project Objective
To build an entire ETL stream from data source all the way through to a Business Intelligence tool.

# Introduction
Extract, Transform, and Load (ETL) processing is critical to modern data management systems. It involves the extraction of large volumes of data from diverse sources in a manner with integrity and consistency, followed by a series of transformations to ensure that the data is appropriately formatted, and finally, the loading of the transformed data into a destination system where it can be accessed and utilized for various purposes, such as reporting, analysis, and decision-making. 

# Description of Project
Data during Covid-19 period was collected from 3 sources: The government of Canada, Ontario Health, and Ottawa Public Health. Then, these csv files were extracted into Python to process data transformation including merging, cleaning, filtering, reformation, joining, and so on. These functions standardized the extracted data into the format we need. After the validation and version control, these data was loaded into a data warehouse (MS SQL Server) where the data was ready for analysis. The data then was loaded into Power BI for data visualization eventually.

